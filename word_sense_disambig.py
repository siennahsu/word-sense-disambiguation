# -*- coding: utf-8 -*-
"""word-sense-disambig.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SW1r7csavhkkGE2q9VRObIlbeZ431u4S

# Word sense disambiguation
"""

!pip install nltk

import nltk
nltk.download('wordnet')
wn = nltk.corpus.wordnet
from nltk.wsd import lesk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
import random
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
import copy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.semi_supervised import LabelSpreading
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.naive_bayes import ComplementNB

"""## Data Preprocessing for baseline and Lesk's models"""

'''
@author: jcheung

Developed for Python 2. Automatically converted to Python 3; may result in bugs.
'''
import xml.etree.cElementTree as ET
import codecs

class WSDInstance:
    def __init__(self, my_id, lemma, context, index):
        self.id = my_id         # id of the WSD instance
        self.lemma = lemma      # lemma of the word whose sense is to be resolved
        self.context = context  # lemma of all the words in the sentential context
        self.index = index      # index of lemma within the context
    def __str__(self):
        '''
        For printing purposes.
        '''
        return '%s\t%s\t%s\t%d' % (self.id, self.lemma, b' '.join(self.context), self.index)

def load_instances(f):
    '''
    Load two lists of cases to perform WSD on. The structure that is returned is a dict, where
    the keys are the ids, and the values are instances of WSDInstance.
    '''
    tree = ET.parse(f)
    root = tree.getroot()

    dev_instances = {}
    test_instances = {}

    for text in root:
        if text.attrib['id'].startswith('d001'):
            instances = dev_instances
        else:
            instances = test_instances
        for sentence in text:
            # construct sentence context
            context = [to_ascii(el.attrib['lemma']) for el in sentence]
            for i, el in enumerate(sentence):
                if el.tag == 'instance':
                    my_id = el.attrib['id']
                    lemma = to_ascii(el.attrib['lemma'])
                    instances[my_id] = WSDInstance(my_id, lemma, context, i)
    return dev_instances, test_instances

def load_key(f):
    '''
    Load the solutions as dicts.
    Key is the id
    Value is the list of correct sense keys.
    '''
    dev_key = {}
    test_key = {}
    for line in open(f):
        if len(line) <= 1: continue
        #print (line)
        doc, my_id, sense_key = line.strip().split(' ', 2)
        if doc == 'd001':
            dev_key[my_id] = sense_key.split()
        else:
            test_key[my_id] = sense_key.split()
    return dev_key, test_key

def to_ascii(s):
    # remove all non-ascii characters
    return codecs.encode(s, 'ascii', 'ignore')

if __name__ == '__main__':
    data_f = 'multilingual-all-words.en.xml'
    key_f = 'wordnet.en.key'
    dev_instances, test_instances = load_instances(data_f)
    dev_key, test_key = load_key(key_f)

    # IMPORTANT: keys contain fewer entries than the instances; need to remove them
    dev_instances = {k:v for (k,v) in dev_instances.items() if k in dev_key}
    test_instances = {k:v for (k,v) in test_instances.items() if k in test_key}

    # read to use here
    print(len(dev_instances)) # number of dev instances
    print(len(test_instances)) # number of test instances

# load the gold standards
with open('/content/wordnet.en.key') as f:
  lines = f.readlines()

# put the gold standard into a list
# each element is of the form (ID, word sense)
gold_standard = []
for line in lines:
    gold_standard.append(tuple(line.split(" ")[1:3]))

stop_words = stopwords.words('english')

# tokenize and lemmatize, remove stop words
# make a list of dev instances, each element is in the form
# (index, word, sentence)

dev_processed = []

for key in dev_instances.keys():
    # get the lemma
    word = dev_instances[key].lemma.decode('utf-8').lower()
    # get the sentence
    sentence = dev_instances[key].context
    # tokenize
    tokens = []
    for w in sentence:
        tokens.append(w.decode('utf-8').lower())
    # lemmatize
    lemmatizer = WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    # remove stop words
    filtered_lemmas = [lemma for lemma in lemmas if lemma not in stop_words]
    # get the index
    index = dev_instances[key].id
    # append to the list
    dev_processed.append((index, word, filtered_lemmas))

# tokenize and lemmatize, remove stop words
# make a list of dev instances, each element is in the form
# (index, word, sentence)

test_processed = []

for key in test_instances.keys():
    # get the lemma
    word = test_instances[key].lemma.decode('utf-8').lower()
    # get the sentence
    sentence = test_instances[key].context
    # tokenize
    tokens = []
    for w in sentence:
        tokens.append(w.decode('utf-8').lower())
    # lemmatize
    lemmatizer = WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    # remove stop words
    filtered_lemmas = [lemma for lemma in lemmas if lemma not in stop_words]
    # get the index
    index = test_instances[key].id
    # append to the list
    test_processed.append((index, word, filtered_lemmas))

"""## Baseline model: The most frequent sense

Go through all the test instances and assign them the most frequent sense.
"""

# go through all the test cases and record the number of accurate word sense assignment
test_size = len(test_instances)
accurate_assignments = 0
for key in test_instances.keys():
    # get the lemma and the word sense
    word = test_instances[key].lemma.decode('utf-8').lower()
    sense_key = wn.synsets(word)[0].lemmas()[0].key()
    sense_key = sense_key.split("%")[1]

    # get the gold standard
    for line in lines:
        if key in line:
            gold_std = line.split(" ")[2]
            gold_std = gold_std.split("%")[1]

    if sense_key == gold_std:
        accurate_assignments += 1

# calculate the accuracy
accuracy = accurate_assignments / test_size
print("Accuracy for baseline model:", accuracy)
print()

"""## NLTK's Lesk's algorithm model

Call NLTK's Lesk's algorithm model to perform word sense disambiguation.
"""

# go through all the test cases and record the number of accurate word sense assignment
test_size = len(test_instances)
accurate_assignments = 0
for test_instance in test_processed:
    key = test_instance[0]

    # get the lemma and the word sense
    word = test_instance[1]
    sense_key = lesk(test_instance[2], word, 'n').lemmas()[0].key()
    sense_key = sense_key.split("%")[1]

    # get the gold standard
    for gs in gold_standard:
        if key == gs[0]:
            gold_std = gs[1]
            gold_std = gold_std.split("%")[1]

    if sense_key == gold_std:
        accurate_assignments += 1

# calculate the accuracy
accuracy = accurate_assignments / test_size
print("Accuracy for Lesk's model:", accuracy)
print()

"""## Data for Semi-supervised and supervised ML models
Here, we choose 5 words to disambiguate. For each word, we build training and test datasets for it. We lemmatize and vectorize the words. Let's choose the 5 words that are in both dev set and test set and have the hightest occurances in the test set.
"""

# words that are in both test and dev sets
overlap_words = []
for instance in test_processed:
    word = instance[1]
    for w in dev_processed:
        if word == w[1]:
            overlap_words.append(word)
            break

# Initialize an empty dictionary to store counts
word_counts = {}

# Count occurrences using a loop
for word in overlap_words:
    if word in word_counts:
        word_counts[word] += 1
    else:
        word_counts[word] = 1

# Get the 8 keys with the highest values
top_8_keys = sorted(word_counts, key=word_counts.get, reverse=True)[:8]

# Output the result
print("Words to disambiguate and their counts in the test set:")
for key in top_8_keys:
    print(f"{key}: {word_counts[key]}")
print()

# see the entries for the words in dev set
for word in top_8_keys:
    for entry in dev_processed:
        if word == entry[1]:
            print(entry)

# see the words in gold standard
for word in top_8_keys:
    for gs in gold_standard:
        if word == gs[1].split("%")[0]:
            print(gs)

# a function that reads the files, construct train and test sets,
# tokenize, lemmatize, remove stop words, and vectorize
# ngram can be "uni", "bi", "tri"

def output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, ngram):
    """
    return X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test
    """

    # opening the files
    with open(sense_1_file) as f:
        sense_1_corpus = f.read()

    with open(sense_2_file) as f:
        sense_2_corpus = f.read()

    with open(general_file) as f:
        unlabeled_corpus = f.read()
        unlabeled_corpus_list = unlabeled_corpus.split("\n")

    # attach the label to the corpus, shuffle it
    labeled_corpus_list = _

    sense_1_list = sense_1_corpus.split("\n")
    for i in range(len(sense_1_list)):
      sense_1_list[i] = (sense_1_list[i], 1)  # 1 means sense 1

    sense_2_list = sense_2_corpus.split("\n")
    for i in range(len(sense_2_list)):
      sense_2_list[i] = (sense_2_list[i], 2)  # 2 means sense 2

    # add the instances in the dev set
    ids_of_target_word = []

    for instance in dev_processed:
        if instance[1] == target_word:
            ids_of_target_word.append(instance[0])

    # retrieve the sentences
    for id in ids_of_target_word:
        # get the sentence
        sentence = ""
        for key in dev_instances.keys():
            if key == id:
                sentence = b' '.join(dev_instances[key].context).decode("utf-8")
        # get the gold standard
        for gs in gold_standard:
            if id == gs[0]:
                gold_sense = gs[1].split("%")[1]
                if gold_sense == sense_1:
                    sense_1_list.append((sentence, 1))
                if gold_sense == sense_2:
                    sense_2_list.append((sentence, 2))

    labeled_corpus_list = sense_1_list + sense_2_list
    random.Random(550).shuffle(labeled_corpus_list)
    print("Size of unlabeled training set: ", len(unlabeled_corpus_list))
    print("Size of labeled training set: ", len(labeled_corpus_list))

    # construct the test set for target word
    labeled_test_cases = []
    ids_of_target_word = []

    for instance in test_processed:
        if instance[1] == target_word:
            ids_of_target_word.append(instance[0])

    # retrieve the sentences
    for id in ids_of_target_word:
        # get the sentence
        sentence = ""
        for key in test_instances.keys():
            if key == id:
                sentence = b' '.join(test_instances[key].context).decode("utf-8")
        # get the gold standard
        for gs in gold_standard:
            if id == gs[0]:
                gold_sense = gs[1].split("%")[1]
                if gold_sense == sense_1:
                    labeled_test_cases.append((sentence, 1))
                if gold_sense == sense_2:
                    labeled_test_cases.append((sentence, 2))

    print("Size of test set (labeled): ", len(labeled_test_cases))

    # develop training and test set

    X_train_labeled = []
    y_train_labeled = []
    for i in range(len(labeled_corpus_list)):
        X_train_labeled.append(labeled_corpus_list[i][0])
        y_train_labeled.append(labeled_corpus_list[i][1])

    X_train_semi = []
    y_train_semi = []

    for i in range(len(unlabeled_corpus_list)):
        X_train_semi.append(unlabeled_corpus_list[i])
        y_train_semi.append(-1)  # -1 means unlabeled

    X_train_semi_l = copy.deepcopy(X_train_labeled)
    X_train_semi += X_train_semi_l
    y_train_semi += y_train_labeled

    X_test = []
    y_test = []
    for i in range(len(labeled_test_cases)):
        X_test.append(labeled_test_cases[i][0])
        y_test.append(labeled_test_cases[i][1])

    # tokenize the words in each datapoint and then lemmatize the words

    wordnet_lemmatizer = WordNetLemmatizer()

    for i in range(len(X_train_semi)):
        X_train_semi[i] = X_train_semi[i].lower()
        X_train_semi[i]= X_train_semi[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ").replace("@", "")
        X_train_semi[i] = word_tokenize(X_train_semi[i])

        for j in range(len(X_train_semi[i])):
          X_train_semi[i][j] = wordnet_lemmatizer.lemmatize(X_train_semi[i][j])

        X_train_semi[i] = " ".join(X_train_semi[i])

    for i in range(len(X_train_labeled)):
        X_train_labeled[i] = X_train_labeled[i].lower()
        X_train_labeled[i]= X_train_labeled[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ").replace("@", "")
        X_train_labeled[i] = word_tokenize(X_train_labeled[i])

        for j in range(len(X_train_labeled[i])):
          X_train_labeled[i][j] = wordnet_lemmatizer.lemmatize(X_train_labeled[i][j])

        X_train_labeled[i] = " ".join(X_train_labeled[i])

    # count-vectorize the words
    # Create a Vectorizer Object
    if ngram == "uni":
        vectorizer = CountVectorizer(ngram_range = (1, 1), stop_words='english')
    elif ngram == "bi":
        vectorizer = CountVectorizer(ngram_range = (2, 2), stop_words='english')
    elif ngram == "tri":
        vectorizer = CountVectorizer(ngram_range = (3, 3), stop_words='english')

    vectorizer.fit(X_train_semi)

    # Encode X_train_semi and X_train_labeled
    X_train_semi = vectorizer.transform(X_train_semi).toarray()
    X_train_labeled = vectorizer.transform(X_train_labeled).toarray()

    # Define a function to streamline lemmatization and vectorization
    def lemma_preprocess(list_of_sen):
        for i in range(len(list_of_sen)):
            list_of_sen[i] = list_of_sen[i].lower()
            list_of_sen[i]= list_of_sen[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ")
            list_of_sen[i] = word_tokenize(list_of_sen[i])

            for j in range(len(list_of_sen[i])):
                list_of_sen[i][j] = wordnet_lemmatizer.lemmatize(list_of_sen[i][j])

            list_of_sen[i] = " ".join(list_of_sen[i])

        return vectorizer.transform(list_of_sen).toarray()

    # Lemmatize and vectorize the test data
    X_test = lemma_preprocess(X_test)

    return X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test

"""## Semi-supervised/Boostrapping model

### Word: country

We pick two senses of country. They are:
1.  S: (n) state (state%1:14:00::), nation (nation%1:14:00::), country (country%1:14:00::), land (land%1:14:00::), commonwealth (commonwealth%1:14:00::), res publica (res_publica%1:14:00::), body politic (body_politic%1:14:00::) (a politically organized body of people under a single government) "the state has elected a new president"; "African nations"; "students who had come to the nation's capitol"; "the country's largest manufacturer"; "an industrialized land".
2.  S: (n) country (country%1:15:01::), rural area (rural_area%1:15:00::) (an area outside of cities and towns) "his poetry celebrated the slower pace of life in the country"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "country"
sense_1_file = "/content/country_nation.txt"
sense_2_file = "/content/country_rural.txt"
general_file = "/content/country.txt"
sense_1 = "1:14:00::"
sense_2 = "1:15:01::"

"""Let's try out unigram, bigram, and trigram models."""

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [3.0, 5.0, 10.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.001, 0.01, 0.05, 0.1],
              'max_iter': [1, 2, 3, 5]}

semi_uni_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_uni_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"country\" with Unigram Proprocessing - Best Hyperparameters\n")
print(semi_uni_model.best_params_)
print()

semi_uni_model_predictions = semi_uni_model.predict(X_test)
semi_uni_report = classification_report(y_test, semi_uni_model_predictions)
print("Semi-Supervised Model \"country\" with Unigram Proprocessing - Performance Report\n")
print(semi_uni_report)
print()

print(semi_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [2.0, 3.0, 4.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.001, 0.01, 0.05, 0.1],
              'max_iter': [1, 2, 3, 5]}

semi_bi_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_bi_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"country\" with Bigram Proprocessing - Best Hyperparameters\n")
print(semi_bi_model.best_params_)
print()

semi_bi_model_predictions = semi_bi_model.predict(X_test)
semi_bi_report = classification_report(y_test, semi_bi_model_predictions)
print("Semi-Supervised Model \"country\" with Bigram Proprocessing - Performance Report\n")
print(semi_bi_report)
print()

print(semi_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [1.0, 2.0, 3.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.00001, 0.0001, .001, 0.01],
              'max_iter': [1, 2, 3]}

semi_tri_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_tri_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"country\" with Trigram Proprocessing - Best Hyperparameters\n")
print(semi_tri_model.best_params_)
print()

semi_tri_model_predictions = semi_tri_model.predict(X_test)
semi_tri_report = classification_report(y_test, semi_tri_model_predictions)
print("Semi-Supervised Model \"country\" with Trigram Proprocessing - Performance Report\n")
print(semi_tri_report)
print()

print(semi_tri_model_predictions)
print(y_test)

"""### Word: game

We pick two senses of game. They are:
1.  S: (n) plot (plot%1:09:00::), secret plan (secret_plan%1:09:00::), game (game%1:09:00::) (a secret scheme to do something (especially something underhand or illegal)) "they concocted a plot to discredit the governor"; "I saw through his little game from the start"
2.  S: (n) game (game%1:04:03::) (a single play of a sport or other contest) "the game lasted two hours"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "game"
sense_1_file = "/content/game_secret.txt"
sense_2_file = "/content/game_contest.txt"
general_file = "/content/game.txt"
sense_1 = "1:09:00::"
sense_2 = "1:04:03::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [8.0, 10.0, 12.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.001, 0.01, 0.05, 0.1],
              'max_iter': [1, 2, 3, 5]}

semi_uni_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_uni_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"game\" with Unigram Proprocessing - Best Hyperparameters\n")
print(semi_uni_model.best_params_)
print()

semi_uni_model_predictions = semi_uni_model.predict(X_test)
semi_uni_report = classification_report(y_test, semi_uni_model_predictions)
print("Semi-Supervised Model \"game\" with Unigram Proprocessing - Performance Report\n")
print(semi_uni_report)
print()

print(semi_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [2.0, 3.0, 4.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.0001, 0.001, 0.01],
              'max_iter': [1, 2, 3, 5]}

semi_bi_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_bi_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"game\" with Bigram Proprocessing - Best Hyperparameters\n")
print(semi_bi_model.best_params_)
print()

semi_bi_model_predictions = semi_bi_model.predict(X_test)
semi_bi_report = classification_report(y_test, semi_bi_model_predictions)
print("Semi-Supervised Model \"game\" with Bigram Proprocessing - Performance Report\n")
print(semi_bi_report)
print()

print(semi_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [1.0, 2.0, 3.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.00001, 0.0001, .001, 0.01],
              'max_iter': [1, 2, 3]}

semi_tri_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_tri_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"game\" with Trigram Proprocessing - Best Hyperparameters\n")
print(semi_tri_model.best_params_)
print()

semi_tri_model_predictions = semi_tri_model.predict(X_test)
semi_tri_report = classification_report(y_test, semi_tri_model_predictions)
print("Semi-Supervised Model \"game\" with Trigram Proprocessing - Performance Report\n")
print(semi_tri_report)
print()

print(semi_tri_model_predictions)
print(y_test)

"""### Word: action

We pick two senses of action. They are:
1.  S: (n) action (action%1:04:02::) (something done (usually as opposed to something said)) "there were stories of murders and other unnatural actions"
2.  S: (n) action (action%1:04:04::) (an act by a government body or supranational organization) "recent federal action undermined the segregationist position"; "the United Nations must have the power to propose and organize action without being hobbled by irrelevant issues"; "the Union action of emancipating Southern slaves"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "action"
sense_1_file = "/content/action_done.txt"
sense_2_file = "/content/action_gov.txt"
general_file = "/content/action.txt"
sense_1 = "1:04:02::"
sense_2 = "1:04:04::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [8.0, 10.0, 12.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.001, 0.01, 0.05, 0.1],
              'max_iter': [1, 2, 3, 5]}

semi_uni_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_uni_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"action\" with Unigram Proprocessing - Best Hyperparameters\n")
print(semi_uni_model.best_params_)
print()

semi_uni_model_predictions = semi_uni_model.predict(X_test)
semi_uni_report = classification_report(y_test, semi_uni_model_predictions)
print("Semi-Supervised Model \"action\" with Unigram Proprocessing - Performance Report\n")
print(semi_uni_report)
print()

print(semi_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [3.0, 4.0, 5.0],
              'n_neighbors': [1, 2, 3],
              'alpha': [0.0001, 0.001, 0.01],
              'max_iter': [1, 2, 3, 5]}

semi_bi_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_bi_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"action\" with Bigram Proprocessing - Best Hyperparameters\n")
print(semi_bi_model.best_params_)
print()

semi_bi_model_predictions = semi_bi_model.predict(X_test)
semi_bi_report = classification_report(y_test, semi_bi_model_predictions)
print("Semi-Supervised Model \"action\" with Bigram Proprocessing - Performance Report\n")
print(semi_bi_report)
print()

print(semi_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [3.0, 4.0, 5.0],
              'n_neighbors': [1, 2, 3],
              'alpha': [0.00001, 0.0001, .001, 0.01],
              'max_iter': [1, 2, 3]}

semi_tri_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_tri_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"action\" with Trigram Proprocessing - Best Hyperparameters\n")
print(semi_tri_model.best_params_)
print()

semi_tri_model_predictions = semi_tri_model.predict(X_test)
semi_tri_report = classification_report(y_test, semi_tri_model_predictions)
print("Semi-Supervised Model \"action\" with Trigram Proprocessing - Performance Report\n")
print(semi_tri_report)
print()

print(semi_tri_model_predictions)
print(y_test)

"""### Word: claim

We pick two senses of claim. They are:
1.  S: (n) claim (claim%1:10:02::) (an assertion that something is true or factual) "his claim that he was innocent"; "evidence contradicted the government's claims"
2.  S: (n) claim (claim%1:04:00::) (demand for something as rightful or due) "they struck in support of their claim for a shorter work day"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "claim"
sense_1_file = "/content/claim_assertion.txt"
sense_2_file = "/content/claim_demand.txt"
general_file = "/content/claim.txt"
sense_1 = "1:10:02::"
sense_2 = "1:04:00::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [10.0, 12.0, 14.0],
              'n_neighbors': [1, 2, 3, 4],
              'alpha': [0.00001, 0.0001, 0.001],
              'max_iter': [1, 2, 3, 5]}

semi_uni_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_uni_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"claim\" with Unigram Proprocessing - Best Hyperparameters\n")
print(semi_uni_model.best_params_)
print()

semi_uni_model_predictions = semi_uni_model.predict(X_test)
semi_uni_report = classification_report(y_test, semi_uni_model_predictions)
print("Semi-Supervised Model \"claim\" with Unigram Proprocessing - Performance Report\n")
print(semi_uni_report)
print()

print(semi_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [1.0, 2.0, 3.0, 4.0],
              'n_neighbors': [1, 2],
              'alpha': [0.00001, 0.0001, 0.001],
              'max_iter': [1, 2, 3]}

semi_bi_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_bi_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"claim\" with Bigram Proprocessing - Best Hyperparameters\n")
print(semi_bi_model.best_params_)
print()

semi_bi_model_predictions = semi_bi_model.predict(X_test)
semi_bi_report = classification_report(y_test, semi_bi_model_predictions)
print("Semi-Supervised Model \"claim\" with Bigram Proprocessing - Performance Report\n")
print(semi_bi_report)
print()

print(semi_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [1.0, 2.0, 3.0, 4.0],
              'n_neighbors': [1, 2],
              'alpha': [0.00001, 0.0001, 0.001],
              'max_iter': [1, 2, 3]}

semi_tri_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_tri_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"claim\" with Trigram Proprocessing - Best Hyperparameters\n")
print(semi_tri_model.best_params_)
print()

semi_tri_model_predictions = semi_tri_model.predict(X_test)
semi_tri_report = classification_report(y_test, semi_tri_model_predictions)
print("Semi-Supervised Model \"claim\" with Trigram Proprocessing - Performance Report\n")
print(semi_tri_report)
print()

print(semi_tri_model_predictions)
print(y_test)

"""### Word: time

We pick two senses of time. They are:
1.  S: (n) time (time%1:11:00::), clip (clip%1:11:00::) (an instance or single occasion for some event) "this time he succeeded"; "he called four times"; "he could do ten at a clip"
2. S: (n) time (time%1:28:05::) (a period of time considered as a resource under your control and sufficient to accomplish something) "take time to smell the roses"; "I didn't have time to finish"; "it took more than half my time"; "he waited for a long time"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "time"
sense_1_file = "/content/time_event.txt"
sense_2_file = "/content/time_resource.txt"
general_file = "/content/time.txt"
sense_1 = "1:11:00::"
sense_2 = "1:28:05::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [8.0, 10.0, 12.0],
              'n_neighbors': [1, 2, 3],
              'alpha': [0.00001, 0.0001, 0.001],
              'max_iter': [1, 2, 3]}

semi_uni_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_uni_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"time\" with Unigram Proprocessing - Best Hyperparameters\n")
print(semi_uni_model.best_params_)
print()

semi_uni_model_predictions = semi_uni_model.predict(X_test)
semi_uni_report = classification_report(y_test, semi_uni_model_predictions)
print("Semi-Supervised Model \"time\" with Unigram Proprocessing - Performance Report\n")
print(semi_uni_report)
print()

print(semi_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [6.0, 8.0, 10.0],
              'n_neighbors': [1, 2, 3],
              'alpha': [0.00001, 0.0001, 0.001],
              'max_iter': [1, 2, 3]}

semi_bi_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_bi_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"time\" with Bigram Proprocessing - Best Hyperparameters\n")
print(semi_bi_model.best_params_)
print()

semi_bi_model_predictions = semi_bi_model.predict(X_test)
semi_bi_report = classification_report(y_test, semi_bi_model_predictions)
print("Semi-Supervised Model \"time\" with Bigram Proprocessing - Performance Report\n")
print(semi_bi_report)
print()

print(semi_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'kernel': ['knn', 'rbf'],
              'gamma': [1.0, 2.0],
              'n_neighbors': [1, 2, 3],
              'alpha': [0.0000001, 0.000001, 0.00001],
              'max_iter': [1, 2, 3]}

semi_tri_model = GridSearchCV(LabelSpreading(), param_grid, refit=True, verbose=3, n_jobs=-1)
semi_tri_model.fit(X_train_semi, y_train_semi)

print("Semi-Supervised Model \"time\" with Trigram Proprocessing - Best Hyperparameters\n")
print(semi_tri_model.best_params_)
print()

semi_tri_model_predictions = semi_tri_model.predict(X_test)
semi_tri_report = classification_report(y_test, semi_tri_model_predictions)
print("Semi-Supervised Model \"time\" with Trigram Proprocessing - Performance Report\n")
print(semi_tri_report)
print()

print(semi_tri_model_predictions)
print(y_test)

"""## Naive Bayes model

### Word: country

We pick two senses of country. They are:
1.  S: (n) state (state%1:14:00::), nation (nation%1:14:00::), country (country%1:14:00::), land (land%1:14:00::), commonwealth (commonwealth%1:14:00::), res publica (res_publica%1:14:00::), body politic (body_politic%1:14:00::) (a politically organized body of people under a single government) "the state has elected a new president"; "African nations"; "students who had come to the nation's capitol"; "the country's largest manufacturer"; "an industrialized land".
2.  S: (n) country (country%1:15:01::), rural area (rural_area%1:15:00::) (an area outside of cities and towns) "his poetry celebrated the slower pace of life in the country"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "country"
sense_1_file = "/content/country_nation.txt"
sense_2_file = "/content/country_rural.txt"
general_file = "/content/country.txt"
sense_1 = "1:14:00::"
sense_2 = "1:15:01::"

"""Let's try out unigram, bigram, and trigram models."""

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_uni_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_uni_model.fit(X_train_labeled, y_train_labeled)

print("Complement Naive Bayes Model \"country\" with Unigram Proprocessing - Best Hyperparameters\n")
print(cnb_uni_model.best_params_)
print()

cnb_uni_model_predictions = cnb_uni_model.predict(X_test)
cnb_uni_report = classification_report(y_test, cnb_uni_model_predictions)
print("Complement Naive Bayes Model \"country\" with Unigram Proprocessing - Performance Report\n")
print(cnb_uni_report)
print()

print(cnb_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_bi_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_bi_model.fit(X_train_labeled, y_train_labeled)

print("Complement Naive Bayes Model \"country\" with Bigram Proprocessing - Best Hyperparameters\n")
print(cnb_bi_model.best_params_)
print()

cnb_bi_model_predictions = cnb_bi_model.predict(X_test)
cnb_bi_report = classification_report(y_test, cnb_bi_model_predictions)
print("Complement Naive Bayes Model \"country\" with Bigram Proprocessing - Performance Report\n")
print(cnb_bi_report)
print()

print(cnb_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_tri_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_tri_model.fit(X_train_labeled, y_train_labeled)

print("Complement Naive Bayes Model \"country\" with Trigram Proprocessing - Best Hyperparameters\n")
print(cnb_tri_model.best_params_)
print()

cnb_tri_model_predictions = cnb_tri_model.predict(X_test)
cnb_tri_report = classification_report(y_test, cnb_tri_model_predictions)
print("Complement Naive Bayes Model \"country\" with Trigram Proprocessing - Performance Report\n")
print(cnb_tri_report)
print()

print(cnb_tri_model_predictions)
print(y_test)

"""### Word: game

We pick two senses of game. They are:
1.  S: (n) plot (plot%1:09:00::), secret plan (secret_plan%1:09:00::), game (game%1:09:00::) (a secret scheme to do something (especially something underhand or illegal)) "they concocted a plot to discredit the governor"; "I saw through his little game from the start"
2.  S: (n) game (game%1:04:03::) (a single play of a sport or other contest) "the game lasted two hours"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "game"
sense_1_file = "/content/game_secret.txt"
sense_2_file = "/content/game_contest.txt"
general_file = "/content/game.txt"
sense_1 = "1:09:00::"
sense_2 = "1:04:03::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_uni_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_uni_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Best Hyperparameters\n")
print(cnb_uni_model.best_params_)
print()

cnb_uni_model_predictions = cnb_uni_model.predict(X_test)
cnb_uni_report = classification_report(y_test, cnb_uni_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Performance Report\n")
print(cnb_uni_report)
print()

print(cnb_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_bi_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_bi_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Best Hyperparameters\n")
print(cnb_bi_model.best_params_)
print()

cnb_bi_model_predictions = cnb_bi_model.predict(X_test)
cnb_bi_report = classification_report(y_test, cnb_bi_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Performance Report\n")
print(cnb_bi_report)
print()

print(cnb_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_tri_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_tri_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Best Hyperparameters\n")
print(cnb_tri_model.best_params_)
print()

cnb_tri_model_predictions = cnb_tri_model.predict(X_test)
cnb_tri_report = classification_report(y_test, cnb_tri_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Performance Report\n")
print(cnb_tri_report)
print()

print(cnb_tri_model_predictions)
print(y_test)

"""### Word: action

We pick two senses of action. They are:
1.  S: (n) action (action%1:04:02::) (something done (usually as opposed to something said)) "there were stories of murders and other unnatural actions"
2.  S: (n) action (action%1:04:04::) (an act by a government body or supranational organization) "recent federal action undermined the segregationist position"; "the United Nations must have the power to propose and organize action without being hobbled by irrelevant issues"; "the Union action of emancipating Southern slaves"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "action"
sense_1_file = "/content/action_done.txt"
sense_2_file = "/content/action_gov.txt"
general_file = "/content/action.txt"
sense_1 = "1:04:02::"
sense_2 = "1:04:04::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_uni_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_uni_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Best Hyperparameters\n")
print(cnb_uni_model.best_params_)
print()

cnb_uni_model_predictions = cnb_uni_model.predict(X_test)
cnb_uni_report = classification_report(y_test, cnb_uni_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Performance Report\n")
print(cnb_uni_report)
print()

print(cnb_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_bi_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_bi_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Best Hyperparameters\n")
print(cnb_bi_model.best_params_)
print()

cnb_bi_model_predictions = cnb_bi_model.predict(X_test)
cnb_bi_report = classification_report(y_test, cnb_bi_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Performance Report\n")
print(cnb_bi_report)
print()

print(cnb_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_tri_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_tri_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Best Hyperparameters\n")
print(cnb_tri_model.best_params_)
print()

cnb_tri_model_predictions = cnb_tri_model.predict(X_test)
cnb_tri_report = classification_report(y_test, cnb_tri_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Performance Report\n")
print(cnb_tri_report)
print()

print(cnb_tri_model_predictions)
print(y_test)

"""### Word: claim

We pick two senses of claim. They are:
1.  S: (n) claim (claim%1:10:02::) (an assertion that something is true or factual) "his claim that he was innocent"; "evidence contradicted the government's claims"
2.  S: (n) claim (claim%1:04:00::) (demand for something as rightful or due) "they struck in support of their claim for a shorter work day"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "claim"
sense_1_file = "/content/claim_assertion.txt"
sense_2_file = "/content/claim_demand.txt"
general_file = "/content/claim.txt"
sense_1 = "1:10:02::"
sense_2 = "1:04:00::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_uni_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_uni_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Best Hyperparameters\n")
print(cnb_uni_model.best_params_)
print()

cnb_uni_model_predictions = cnb_uni_model.predict(X_test)
cnb_uni_report = classification_report(y_test, cnb_uni_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Performance Report\n")
print(cnb_uni_report)
print()

print(cnb_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_bi_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_bi_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Best Hyperparameters\n")
print(cnb_bi_model.best_params_)
print()

cnb_bi_model_predictions = cnb_bi_model.predict(X_test)
cnb_bi_report = classification_report(y_test, cnb_bi_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Performance Report\n")
print(cnb_bi_report)
print()

print(cnb_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_tri_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_tri_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Best Hyperparameters\n")
print(cnb_tri_model.best_params_)
print()

cnb_tri_model_predictions = cnb_tri_model.predict(X_test)
cnb_tri_report = classification_report(y_test, cnb_tri_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Performance Report\n")
print(cnb_tri_report)
print()

print(cnb_tri_model_predictions)
print(y_test)

"""### Word: time

We pick two senses of time. They are:
1.  S: (n) time (time%1:11:00::), clip (clip%1:11:00::) (an instance or single occasion for some event) "this time he succeeded"; "he called four times"; "he could do ten at a clip"
2. S: (n) time (time%1:28:05::) (a period of time considered as a resource under your control and sufficient to accomplish something) "take time to smell the roses"; "I didn't have time to finish"; "it took more than half my time"; "he waited for a long time"

We assign 1 to the first sense and 2 to the second sense.
"""

# parameters to change
target_word = "time"
sense_1_file = "/content/time_event.txt"
sense_2_file = "/content/time_resource.txt"
general_file = "/content/time.txt"
sense_1 = "1:11:00::"
sense_2 = "1:28:05::"

# unigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "uni")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_uni_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_uni_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Best Hyperparameters\n")
print(cnb_uni_model.best_params_)
print()

cnb_uni_model_predictions = cnb_uni_model.predict(X_test)
cnb_uni_report = classification_report(y_test, cnb_uni_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Unigram Proprocessing - Performance Report\n")
print(cnb_uni_report)
print()

print(cnb_uni_model_predictions)
print(y_test)

# bigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "bi")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_bi_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_bi_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Best Hyperparameters\n")
print(cnb_bi_model.best_params_)
print()

cnb_bi_model_predictions = cnb_bi_model.predict(X_test)
cnb_bi_report = classification_report(y_test, cnb_bi_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Bigram Proprocessing - Performance Report\n")
print(cnb_bi_report)
print()

print(cnb_bi_model_predictions)
print(y_test)

# trigram
X_train_semi, y_train_semi, X_train_labeled, y_train_labeled, X_test, y_test = output_train_test_sets(target_word, sense_1_file, sense_2_file, sense_1, sense_2, "tri")

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

cnb_tri_model = GridSearchCV(ComplementNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
cnb_tri_model.fit(X_train_labeled, y_train_labeled)

print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Best Hyperparameters\n")
print(cnb_tri_model.best_params_)
print()

cnb_tri_model_predictions = cnb_tri_model.predict(X_test)
cnb_tri_report = classification_report(y_test, cnb_tri_model_predictions)
print(f"Complement Naive Bayes Model \"{target_word}\" with Trigram Proprocessing - Performance Report\n")
print(cnb_tri_report)
print()

print(cnb_tri_model_predictions)
print(y_test)

"""## Baseline model's performance on the five words"""

words = ["country", "game", "action", "claim", "time"]

# get the accuracies of the baseline model on the five specific words
for target_word in words:
    # go through all the test cases of a word
    test_size = 0
    accurate_assignments = 0
    for key in test_instances.keys():
        # get the lemma and the word sense
        word = test_instances[key].lemma.decode('utf-8').lower()
        if word == target_word:
            test_size += 1

            # get the sense key
            sense_key = wn.synsets(word)[0].lemmas()[0].key()
            sense_key = sense_key.split("%")[1]

            # get the gold standard
            for line in lines:
                if key in line:
                    gold_std = line.split(" ")[2]
                    gold_std = gold_std.split("%")[1]

            if sense_key == gold_std:
                accurate_assignments += 1
    accuracy = accurate_assignments / test_size
    print(f"Accuracy for baseline model on word \"{target_word}\": ", accuracy)

# print the prediction of the baseline model
for target_word in words:
    sense_key = wn.synsets(target_word)[0].lemmas()[0].key()
    sense_key = sense_key.split("%")[1]
    print(f"Baseline model's prediction for {target_word}: {sense_key}")

"""## Lesk's model's performance on the five words"""

# get the accuracies of the Lesk's model on the five specific words
for target_word in words:
    # go through all the test cases and record the number of accurate word sense assignment
    test_size = 0
    accurate_assignments = 0
    for test_instance in test_processed:
        key = test_instance[0]

        # get the lemma and the word sense
        if test_instance[1] == target_word:
            test_size += 1

            # get the sense key
            word = test_instance[1]
            sense_key = lesk(test_instance[2], word, 'n').lemmas()[0].key()
            sense_key = sense_key.split("%")[1]

            # get the gold standard
            for gs in gold_standard:
                if key == gs[0]:
                    gold_std = gs[1]
                    gold_std = gold_std.split("%")[1]

            if sense_key == gold_std:
                accurate_assignments += 1
    accuracy = accurate_assignments / test_size
    print(f"Accuracy for Lesk's model on word \"{target_word}\": ", accuracy)

# print the prediction for each test case
for target_word in words:
    # go through all the test cases and record the number of accurate word sense assignment
    print(f"Target word: {target_word}")
    print()
    test_size = 0
    accurate_assignments = 0
    for test_instance in test_processed:
        key = test_instance[0]

        # get the lemma and the word sense
        if test_instance[1] == target_word:
            test_size += 1
            print(f"\tTest case: {test_instance}")

            # get the sense key
            word = test_instance[1]
            sense_key = lesk(test_instance[2], word, 'n').lemmas()[0].key()
            sense_key = sense_key.split("%")[1]
            print(f"\tPredicted sense key: {sense_key}")

            # get the gold standard
            for gs in gold_standard:
                if key == gs[0]:
                    gold_std = gs[1]
                    gold_std = gold_std.split("%")[1]
                    print(f"\tGold standard sense key: {gold_std}")
                    print()
            if sense_key == gold_std:
                accurate_assignments += 1
    accuracy = accurate_assignments / test_size
    print()

for target_word in words:
    for test_instance in test_processed:
        if test_instance[1] == target_word:
            id = test_instance[0]
            for id_key in test_instances.keys():
                if id_key == id:
                    print(str(test_instances[id_key]))